{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FocalLoss.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "qXRAwoFG1zyY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Focal Loss"
      ]
    },
    {
      "metadata": {
        "id": "4D05-bc2rXyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvHD48rg14mZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cross Entropy Loss\n",
        "\n",
        "For Understanding Focal Loss we need to understand Cross Entropy Loss\n",
        "\n",
        "In pytorch Cross Entropy Loss = LogSoftmax + NLLLoss\n",
        "\n",
        "NLLLoss is negative log likelihood which is used in multiclass classifier.\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\mathcal{L}}=-\\frac{1}{n}\\sum_{i=1}^{n}\\log(\\hat{y}^{(i)})\n",
        "$$\n"
      ]
    },
    {
      "metadata": {
        "id": "7MGRa1lj3SgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Below is the code to understand NLL Loss https://isaacchanghau.github.io/post/loss_functions/"
      ]
    },
    {
      "metadata": {
        "id": "AfDLlaZIbzSA",
        "colab_type": "code",
        "outputId": "068f5032-eb59-4636-93a1-331b02dc87cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "def NLLLoss(logs, targets):\n",
        "    out = torch.zeros_like(targets, dtype=torch.float)\n",
        "    for i in range(len(targets)):\n",
        "        out[i] = logs[i][targets[i]]\n",
        "    return -out.mean()\n",
        "\n",
        "x = torch.randn(3, 5)\n",
        "y = torch.LongTensor([4, 1, 2])\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "log_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "x_log = log_softmax(x)\n",
        "\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "print(\"Torch CrossEntropyLoss: \", cross_entropy_loss(x, y))\n",
        "print(\"Torch NLL loss: \", nll_loss(x_log, y))\n",
        "print(\"Custom NLL loss: \", NLLLoss(x_log, y))\n",
        "# Torch CrossEntropyLoss:  tensor(1.8739)\n",
        "# Torch NLL loss:  tensor(1.8739)\n",
        "# Custom NLL loss:  tensor(1."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch CrossEntropyLoss:  tensor(1.8096)\n",
            "Torch NLL loss:  tensor(1.8096)\n",
            "Custom NLL loss:  tensor(1.8096)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y9yB9THmX13y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Binary Cross Entropy\n",
        " $$\n",
        " \\boldsymbol{\\mathcal{L}} = -y*log(p) - (1-y)*log(1-p)  \n",
        " $$\n",
        " p is the probability and y is true label"
      ]
    },
    {
      "metadata": {
        "id": "AgvGBIjH3Res",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [Focal Loss](https://arxiv.org/pdf/1708.02002.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "CgoeiLRqs_Mt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Binary Focal Loss"
      ]
    },
    {
      "metadata": {
        "id": "0R9Vpp6XYii_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def binary_focal_loss(y_pred , y_true,gamma=2.0 , alpha=0.25 ,reduction=\"mean\",function=torch.sigmoid,**kwargs):\n",
        "    \"\"\"\n",
        "    Binary Version of Focal Loss\n",
        "    :args\n",
        "    \n",
        "    y_pred : prediction\n",
        "    \n",
        "    y_true : true target labels\n",
        "    \n",
        "    gamma: dampeing factor default value 2 works well according to reasearch paper\n",
        "    \n",
        "    alpha : postive to negative ratio default value 0.25 means 1 positive and 3 negative can be tuple ,list ,int and float\n",
        "    \n",
        "    reduction = mean,sum,none\n",
        "\n",
        "    function = can be sigmoid or softmax or None\n",
        "    \n",
        "    **kwargs: parameters to pass in activation function like dim in softmax\n",
        "    \n",
        "    \"\"\"\n",
        "    if isinstance(alpha,(list,tuple)):\n",
        "        pos_alpha = alpha[0] # postive sample ratio in the entire dataset\n",
        "        neg_alpha = alpha[1] #(1-alpha) # negative ratio in the entire dataset\n",
        "    elif isinstance(alpha ,(int,float)):\n",
        "        pos_alpha = alpha\n",
        "        neg_alpha = (1-alpha)\n",
        "        \n",
        "    # if else in function can be simplified be removing setting to default to sigmoid  for educational purpose\n",
        "    if function is not None:\n",
        "        y_pred = function(y_pred , **kwargs) #apply activation function\n",
        "    else :\n",
        "        assert ((y_pred <= 1) & (y_pred >= 0)).all().item() , \"negative value in y_pred value should be in the range of 0 to 1 inclusive\"\n",
        "        \n",
        "        \n",
        "    pos_pt = torch.where(y_true==1 , y_pred , torch.ones_like(y_pred)) # positive pt (fill all the 0 place in y_true with 1 so (1-pt)=0 and log(pt)=0.0) where pt is 1\n",
        "    neg_pt = torch.where(y_true==0 , y_pred , torch.zeros_like(y_pred)) # negative pt\n",
        "    \n",
        "    pos_modulating = (1-pos_pt)**gamma # compute postive modulating factor for correct classification the value approaches to zero\n",
        "    neg_modulating = (neg_pt)**gamma # compute negative modulating factor\n",
        "    \n",
        "    \n",
        "    pos = - pos_alpha*pos_modulating*torch.log(pos_pt) #pos part\n",
        "    neg = - neg_alpha*neg_modulating*torch.log(1-neg_pt) # neg part\n",
        "    \n",
        "    loss = pos+neg  # this is final loss to be returned with some reduction\n",
        "    \n",
        "    # apply reduction\n",
        "    if reduction ==\"mean\":\n",
        "        return loss.mean()\n",
        "    elif reduction ==\"sum\":\n",
        "        return loss.sum()\n",
        "    elif reduction ==\"none\":\n",
        "        return loss # reduction mean\n",
        "    else:\n",
        "        raise f\"Wrong reduction {reduction} is choosen \\n choose one among [mean,sum,none]  \"\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "d039e9fc-5d8f-4656-9f1b-7d31a0305d2d",
        "id": "n4IP6CxeignN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = torch.randn(32,10)\n",
        "y_true = torch.empty(32, 10).random_(2)\n",
        "F.binary_cross_entropy_with_logits(y_pred,y_true) , binary_focal_loss(y_pred,y_true,gamma=0,alpha=[1,1]) # to test the correctness of method"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.8163), tensor(0.8163))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "zmYIODDctDig",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Categorical Focal Loss\n",
        "\n",
        "This focal loss is written for only object detection is SSD and YOLO where we assume that 75% anchors are background and 25% of them are foreground"
      ]
    },
    {
      "metadata": {
        "id": "qo8kiV3mtC-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def categorical_focal_loss(y_pred , y_true,gamma=2.0 , alpha=[0.25 ,0.75] ,reduction=\"mean\"):\n",
        "    \"\"\"\n",
        "    Categorical Version of Focal Loss\n",
        "    :args\n",
        "    \n",
        "    y_pred : prediction\n",
        "    \n",
        "    y_true : true target labels\n",
        "    \n",
        "    gamma: dampeing factor default value 2 works well according to reasearch paper\n",
        "    \n",
        "    alpha : foreground to background ratio can be list,tuple ,int or float \n",
        "    \n",
        "    reduction = mean,sum,none\n",
        "   \n",
        "    \"\"\"\n",
        "    if isinstance(alpha,(list,tuple)):\n",
        "        fore_alpha = alpha[0] # postive sample ratio in the entire dataset\n",
        "        back_alpha = alpha[1] #(1-alpha) # negative ratio in the entire dataset\n",
        "    elif isinstance(alpha ,(int,float)):\n",
        "        fore_alpha = alpha\n",
        "        back_alpha = (1-alpha)\n",
        "\n",
        "    y_true = torch.eye(y_pred.shape[-1])[y_true] # generate one hot vector\n",
        "    \n",
        "    y_pred = F.softmax(y_pred , dim=1) #apply activation function\n",
        "\n",
        "    loss = - y_true * torch.log(y_pred) # cross entropy\n",
        "    loss =  loss * (1 - y_pred) ** gamma # focal loss\n",
        "    \n",
        "    # A common method for addressing class imbalance is to introduce a weighting factor α∈[0,1]for class1 and 1−α for class−1\n",
        "    loss[loss[:,0] == 0.0] = loss[loss[:,0] == 0.0] * fore_alpha # foreground weightage \n",
        "    loss[loss[:,0] != 0.0] = loss[loss[:,0] != 0.0] * back_alpha # background weightage \n",
        "    \n",
        "    loss = loss.sum(dim=1)\n",
        "    # apply reduction\n",
        "    if reduction ==\"mean\":\n",
        "        return loss.mean()\n",
        "    elif reduction ==\"sum\":\n",
        "        return loss.sum()\n",
        "    elif reduction ==\"none\":\n",
        "        return loss # reduction mean\n",
        "    else:\n",
        "        raise f\"Wrong reduction {reduction} is choosen \\n choose one among [mean,sum,none]  \"\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hkmp1rYD94fi",
        "colab_type": "code",
        "outputId": "61947fb8-1b1b-43bf-bef2-4846a5fdbcb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "# Generating SSD data with random configuration\n",
        "\"\"\"\n",
        "confidence_prediction: (batch_size,num_anchors,num_classes) \n",
        "\"\"\"\n",
        "confidence_prediction = torch.randn((32,8732,21),requires_grad=True)\n",
        "\n",
        "\"\"\"\n",
        "target: (batch_size,num_anchors,num_classes)\n",
        "\"\"\"\n",
        "target = torch.empty(32,8732).random_(21).to(torch.long)\n",
        "\n",
        "## Reshaping \n",
        "confidence_prediction = confidence_prediction.view(-1,21)\n",
        "print(f\"confidence_prediction shape {confidence_prediction.shape}\")\n",
        "target = target.view(-1)\n",
        "print(f\"target shape {target.shape}\")\n",
        "\n",
        "target[:209568] = 0 # filling 75% of target with zeros 0.75*target.shape[0] = 209568\n",
        "\n",
        "F.cross_entropy(confidence_prediction,target), categorical_focal_loss(confidence_prediction,target)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confidence_prediction shape torch.Size([279424, 21])\n",
            "target shape torch.Size([279424])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(3.5091, grad_fn=<NllLossBackward>),\n",
              " tensor(2.0646, grad_fn=<MeanBackward1>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "VBJ1epHfBuUO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}