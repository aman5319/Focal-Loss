{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FocalLoss.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aman5319/Focal-Loss/blob/master/FocalLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qXRAwoFG1zyY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Focal Loss"
      ]
    },
    {
      "metadata": {
        "id": "4D05-bc2rXyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VvHD48rg14mZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cross Entropy Loss\n",
        "\n",
        "For Understanding Focal Loss we need to understand Cross Entropy Loss\n",
        "\n",
        "In pytorch Cross Entropy Loss = LogSoftmax + NLLLoss\n",
        "\n",
        "NLLLoss is negative log likelihood which is used in multiclass classifier.\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\mathcal{L}}=-\\frac{1}{n}\\sum_{i=1}^{n}\\log(\\hat{y}^{(i)})\n",
        "$$\n"
      ]
    },
    {
      "metadata": {
        "id": "7MGRa1lj3SgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Below is the code to understand NLL Loss https://isaacchanghau.github.io/post/loss_functions/"
      ]
    },
    {
      "metadata": {
        "id": "AfDLlaZIbzSA",
        "colab_type": "code",
        "outputId": "ed81b84a-b165-481d-f19c-1feb12458aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "def NLLLoss(logs, targets):\n",
        "    out = torch.zeros_like(targets, dtype=torch.float)\n",
        "    for i in range(len(targets)):\n",
        "        out[i] = logs[i][targets[i]]\n",
        "    return -out.mean()\n",
        "\n",
        "x = torch.randn(3, 5)\n",
        "y = torch.LongTensor([4, 1, 2])\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "log_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "x_log = log_softmax(x)\n",
        "\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "print(\"Torch CrossEntropyLoss: \", cross_entropy_loss(x, y))\n",
        "print(\"Torch NLL loss: \", nll_loss(x_log, y))\n",
        "print(\"Custom NLL loss: \", NLLLoss(x_log, y))\n",
        "# Torch CrossEntropyLoss:  tensor(1.8739)\n",
        "# Torch NLL loss:  tensor(1.8739)\n",
        "# Custom NLL loss:  tensor(1."
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch CrossEntropyLoss:  tensor(1.7132)\n",
            "Torch NLL loss:  tensor(1.7132)\n",
            "Custom NLL loss:  tensor(1.7132)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y9yB9THmX13y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Binary Cross Entropy\n",
        " $$\n",
        " \\boldsymbol{\\mathcal{L}} = -y*log(p) - (1-y)*log(1-p)  \n",
        " $$\n",
        " p is the probability and y is true label"
      ]
    },
    {
      "metadata": {
        "id": "AgvGBIjH3Res",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [Focal Loss](https://arxiv.org/pdf/1708.02002.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "CgoeiLRqs_Mt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Binary Focal Loss"
      ]
    },
    {
      "metadata": {
        "id": "0R9Vpp6XYii_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def binary_focal_loss(y_pred , y_true,gamma=2.0 , alpha=0.25 ,reduction=\"mean\",function=torch.sigmoid,**kwargs):\n",
        "    \"\"\"\n",
        "    Binary Version of Focal Loss\n",
        "    :args\n",
        "    \n",
        "    y_pred : prediction\n",
        "    \n",
        "    y_true : true target labels\n",
        "    \n",
        "    gamma: dampeing factor default value 2 works well according to reasearch paper\n",
        "    \n",
        "    alpha : postive to negative ratio default value 0.25 means 1 positive and 3 negative can be tuple ,list ,int and float\n",
        "    \n",
        "    reduction = mean,sum,none\n",
        "\n",
        "    function = can be sigmoid or softmax or None\n",
        "    \n",
        "    **kwargs: parameters to pass in activation function like dim in softmax\n",
        "    \n",
        "    \"\"\"\n",
        "    if isinstance(alpha,(list,tuple)):\n",
        "        pos_alpha = alpha[0] # postive sample ratio in the entire dataset\n",
        "        neg_alpha = alpha[1] #(1-alpha) # negative ratio in the entire dataset\n",
        "    elif isinstance(alpha ,(int,float)):\n",
        "        pos_alpha = alpha\n",
        "        neg_alpha = (1-alpha)\n",
        "        \n",
        "    # if else in function can be simplified be removing setting to default to sigmoid  for educational purpose\n",
        "    if function is not None:\n",
        "        y_pred = function(y_pred , **kwargs) #apply activation function\n",
        "    else :\n",
        "        assert ((y_pred <= 1) & (y_pred >= 0)).all().item() , \"negative value in y_pred value should be in the range of 0 to 1 inclusive\"\n",
        "    \n",
        "    pos_pt = torch.where(y_true==1 , y_pred , torch.ones_like(y_pred)) # positive pt (fill all the 0 place in y_true with 1 so (1-pt)=0 and log(pt)=0.0) where pt is 1\n",
        "    neg_pt = torch.where(y_true==0 , y_pred , torch.zeros_like(y_pred)) # negative pt\n",
        "    \n",
        "    pos_modulating = (1-pos_pt)**gamma # compute postive modulating factor for correct classification the value approaches to zero\n",
        "    neg_modulating = (neg_pt)**gamma # compute negative modulating factor\n",
        "    \n",
        "    \n",
        "    pos = -pos_alpha* pos_modulating*torch.log(pos_pt) #pos part\n",
        "    neg = -neg_alpha* neg_modulating*torch.log(1-neg_pt) # neg part\n",
        "    \n",
        "    loss = pos+neg  # this is final loss to be returned with some reduction\n",
        "    \n",
        "    # apply reduction\n",
        "    if reduction ==\"mean\":\n",
        "        return loss.mean()\n",
        "    elif reduction ==\"sum\":\n",
        "        return loss.sum()\n",
        "    elif reduction ==\"none\":\n",
        "        return loss # reduction mean\n",
        "    else:\n",
        "        raise f\"Wrong reduction {reduction} is choosen \\n choose one among [mean,sum,none]  \"\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "bfbbad5b-9bc4-491f-856d-36583d301935",
        "id": "n4IP6CxeignN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = torch.randn(32,2)\n",
        "y_true = torch.empty(32, 2).random_(2)\n",
        "F.binary_cross_entropy_with_logits(y_pred,y_true) , binary_focal_loss(y_pred,y_true,gamma=0,alpha=[1,1]) # to test the correctness of method"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.8058), tensor(0.8058))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "id": "nBFl7xijeCYx",
        "colab_type": "code",
        "outputId": "88b0c1b7-a3b1-432c-b2d2-304351347765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "binary_focal_loss(y_pred,y_true) # to test the correctness of method"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1338)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "metadata": {
        "id": "zmYIODDctDig",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Categorical Focal Loss\n",
        "\n",
        "This focal loss is written for only object detection is SSD and YOLO where we assume that 75% anchors are background and 25% of them are foreground"
      ]
    },
    {
      "metadata": {
        "id": "qo8kiV3mtC-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def categorical_focal_loss(y_pred , y_true,gamma=2.0 , alpha=[0.25 ,0.75] ,reduction=\"mean\"):\n",
        "    \"\"\"\n",
        "    Categorical Version of Focal Loss\n",
        "    :args\n",
        "    \n",
        "    y_pred : prediction\n",
        "    \n",
        "    y_true : true target labels\n",
        "    \n",
        "    gamma: dampeing factor default value 2 works well according to reasearch paper\n",
        "    \n",
        "    alpha : foreground to background ratio can be list,tuple ,int or float \n",
        "    \n",
        "    reduction = mean,sum,none\n",
        "   \n",
        "    \"\"\"\n",
        "    if isinstance(alpha,(list,tuple)):\n",
        "        fore_alpha = alpha[0] # postive sample ratio in the entire dataset\n",
        "        back_alpha = alpha[1] #(1-alpha) # negative ratio in the entire dataset\n",
        "    elif isinstance(alpha ,(int,float)):\n",
        "        fore_alpha = alpha\n",
        "        back_alpha = (1-alpha)\n",
        "    \n",
        "    y_pred = F.softmax(y_pred , dim=1) #apply activation function\n",
        "        \n",
        "    alpha = torch.tensor([back_alpha , *[fore_alpha]*(y_pred.shape[-1]-1)]) # create alpha.shape= (21,) alpha = [0.75,0.25,0.25,...]\n",
        "    alpha_t = alpha[y_true].cuda()\n",
        "    \n",
        "    pt = (y_pred * torch.eye(y_pred.shape[-1])[y_true].cuda()).sum(1) ##calculate pt\n",
        "    \n",
        "    loss = -1 * alpha_t * ((1-pt)**gamma) * torch.log(pt) # apply the formula\n",
        "    \n",
        "    # apply reduction\n",
        "    if reduction ==\"mean\":\n",
        "        return loss.mean()\n",
        "    elif reduction ==\"sum\":\n",
        "        return loss.sum()\n",
        "    elif reduction ==\"none\":\n",
        "        return loss # reduction mean\n",
        "    else:\n",
        "        raise f\"Wrong reduction {reduction} is choosen \\n choose one among [mean,sum,none]  \""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hkmp1rYD94fi",
        "colab_type": "code",
        "outputId": "f5fcd7bc-4edc-4fe0-faef-76dd41fd54db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "# Generating SSD data with random configuration\n",
        "\"\"\"\n",
        "confidence_prediction: (batch_size,num_anchors,num_classes) \n",
        "\"\"\"\n",
        "confidence_prediction = torch.randn((32,8732,21),requires_grad=True)\n",
        "\n",
        "\"\"\"\n",
        "target: (batch_size,num_anchors,num_classes)\n",
        "\"\"\"\n",
        "target = torch.empty(32,8732).random_(21).to(torch.long)\n",
        "\n",
        "## Reshaping \n",
        "confidence_prediction = confidence_prediction.view(-1,21)\n",
        "print(f\"confidence_prediction shape {confidence_prediction.shape}\")\n",
        "target = target.view(-1)\n",
        "print(f\"target shape {target.shape}\")\n",
        "\n",
        "target[:209568] = 0 # filling 75% of target with zeros 0.75*target.shape[0] = 209568\n",
        "\n",
        "###############cuda##########################\n",
        "target  = target.cuda()\n",
        "confidence_prediction = confidence_prediction.cuda()\n",
        "\n",
        "\n",
        "F.cross_entropy(confidence_prediction,target), categorical_focal_loss(confidence_prediction,target)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confidence_prediction shape torch.Size([279424, 21])\n",
            "target shape torch.Size([279424])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(3.5078, device='cuda:0', grad_fn=<NllLossBackward>),\n",
              " tensor(2.0621, device='cuda:0', grad_fn=<MeanBackward1>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "metadata": {
        "id": "5505ph--DXUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}